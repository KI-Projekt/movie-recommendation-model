{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (24.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-surprise in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-surprise) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\zimmerai\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-surprise) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install pandas\n",
    "%pip install scikit-surprise\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from surprise import Dataset, Reader, KNNBasic,accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "DATA_DIR = '../data'\n",
    "DATA_FILE = 'ml-latest.zip'\n",
    "\n",
    "data_path = os.path.join(DATA_DIR, DATA_FILE)\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    urllib.request.urlretrieve(DATA_URL, data_path)\n",
    "    with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Define the paths to the files\n",
    "    movies_path = os.path.join(DATA_DIR, 'ml-latest-small', 'movies.csv')\n",
    "    ratings_path = os.path.join(DATA_DIR, 'ml-latest-small', 'ratings.csv')\n",
    "    tags_path = os.path.join(DATA_DIR, 'ml-latest-small', 'tags.csv')\n",
    "    links_path = os.path.join(DATA_DIR, 'ml-latest-small', 'links.csv')\n",
    "\n",
    "    # Load the data into pandas DataFrames\n",
    "    movies_df = pd.read_csv(movies_path)\n",
    "    ratings_df = pd.read_csv(ratings_path)\n",
    "    tags_df = pd.read_csv(tags_path)\n",
    "    links_df = pd.read_csv(links_path)\n",
    "    \n",
    "    return movies_df, ratings_df, tags_df, links_df\n",
    "\n",
    "def load_data_as_dataset(df, reader):\n",
    "    \"\"\"\n",
    "    Lädt die Daten in ein Surprise Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "        DataFrame mit den Bewertungsdaten.\n",
    "    reader : Reader\n",
    "        Ein Reader-Objekt von Surprise.\n",
    "\n",
    "    Returns:\n",
    "    full_data : Trainset\n",
    "        Das vollständige Trainset.\n",
    "    train_set : list\n",
    "        Die Trainingsdaten.\n",
    "    test_set : list\n",
    "        Die Testdaten.\n",
    "    \"\"\"\n",
    "    data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "    full_data = data.build_full_trainset()\n",
    "    train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    return full_data, train_set, test_set\n",
    "\n",
    "# Laden der Daten\n",
    "movies_df, ratings_df, tags_df, links_df = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "What data exploration methods do we need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based\n",
    "This is a function to train a content based recommendation model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_content_based_model(movies, ratings, tags, links) :\n",
    "    print(\"Training content-based model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering - Neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "2\n",
      "[(78499, 4.112729059889114), (3114, 3.8492793176134312), (1, 3.8375), (2, 3.613428389547348), (3, 3.3112346367300605), (5, 3.0055588221545286), (4, 2.3582630827718467)]\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9707\n",
      "MAE:  0.7479\n",
      "RMSE: 0.9707148710376821, MAE: 0.7479303056189172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Wir müssen bei jedem neuen Nutzer neu trainieren --> nicht effizient\n",
    "#Das Dataset ist zu groß für die Speicherung in einer Variable -> Speicherprobleme bei großen Datensätzen -> nicht möglich\n",
    "user = [\n",
    "    {\"movieId\": 1, \"rating\": 5},\n",
    "    {\"movieId\": 2, \"rating\": 3},\n",
    "]\n",
    "\n",
    "movie_list = [1, 2, 3, 4, 5, 3114, 78499]\n",
    "\n",
    "def train_model(ratings):\n",
    "    from surprise import KNNBasic, Dataset, Reader\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    model = KNNBasic(sim_options={'name': 'cosine', 'user_based': True})\n",
    "    model.fit(trainset)\n",
    "    return model\n",
    "\n",
    "def get_nearest_user(model, user_ratings, ratings):\n",
    "\n",
    "    # Convert user_ratings to Surprise dataset format\n",
    "    ratings = pd.DataFrame(user_ratings)\n",
    "    ratings['userId'] = 'new_user'\n",
    "    reader = Reader(rating_scale=(1, 5))\n",
    "    user_data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "    user_trainset = user_data.build_full_trainset()\n",
    "    user_inner_id = user_trainset.to_inner_uid('new_user')\n",
    "\n",
    "    # Find the nearest neighbor to the new user\n",
    "    nearest_neighbors = model.get_neighbors(user_inner_id, k=1)\n",
    "    nearest_user_id = model.trainset.to_raw_uid(nearest_neighbors[0])\n",
    "\n",
    "    # Return the original userId of the nearest neighbor\n",
    "    return nearest_user_id\n",
    "\n",
    "\n",
    "def predict_ratings(model, nearest_user_id, movie_list):\n",
    "    predictions = []\n",
    "    for movie_id in movie_list:\n",
    "        prediction = model.predict(nearest_user_id, movie_id)\n",
    "        predictions.append((movie_id, prediction.est))\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions\n",
    "\n",
    "# Assuming `ratings_df` is your DataFrame containing the ratings\n",
    "model = train_model(ratings_df)\n",
    "nearest_user_id = get_nearest_user(model, user, ratings_df)\n",
    "print(nearest_user_id)\n",
    "predicted_ratings = predict_ratings(model, nearest_user_id, movie_list)\n",
    "print(predicted_ratings)\n",
    "\n",
    "# Assuming [`ratings_df`](command:_github.copilot.openSymbolFromReferences?%5B%7B%22%24mid%22%3A1%2C%22fsPath%22%3A%22d%3A%5C%5Cdev%5C%5Cmovie-recommendation-model%5C%5Cnotebooks%5C%5Cmovie-recommendations.ipynb%22%2C%22_sep%22%3A1%2C%22path%22%3A%22%2Fd%3A%2Fdev%2Fmovie-recommendation-model%2Fnotebooks%2Fmovie-recommendations.ipynb%22%2C%22scheme%22%3A%22vscode-notebook-cell%22%2C%22fragment%22%3A%22X10sZmlsZQ%3D%3D%22%7D%2C%7B%22line%22%3A39%2C%22character%22%3A11%7D%5D \"d:\\dev\\movie-recommendation-model\\notebooks\\movie-recommendations.ipynb\") is your DataFrame containing the ratings\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Split the data into training and test set (e.g., 75% training, 25% testing)\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Train the model on the training set\n",
    "model = KNNBasic(sim_options={'name': 'cosine', 'user_based': True})\n",
    "model.fit(trainset)\n",
    "\n",
    "# Predict ratings for the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Compute and print the accuracy of the model\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "\n",
    "#RMSE (Root Mean Square Error): Measures the average magnitude of the errors in a set of predictions, without considering their direction. Lower RMSE values indicate better fit.\n",
    "#MAE (Mean Absolute Error): Measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s a linear score, which means all the individual differences are weighted equally. Lower MAE values indicate better fit.\n",
    "print(f\"RMSE: {rmse}, MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering - Matrix Factorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_matrix_factorization_model(movies, ratings, tags, links):\n",
    "    print(\"Training matrix factorization model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training content-based model\n",
      "Training neighborhood model\n",
      "Training matrix factorization model\n"
     ]
    }
   ],
   "source": [
    "def train_models(movies, ratings, tags, links) :\n",
    "    train_content_based_model(movies, ratings, tags, links)\n",
    "    train_neighborhood_model(movies, ratings, tags, links)\n",
    "    train_matrix_factorization_model(movies, ratings, tags, links)\n",
    "\n",
    "movies_df, ratings_df, tags_df, links_df = load_data()\n",
    "train_models(movies=movies_df, ratings=ratings_df, tags=tags_df, links=links_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should return a list of recommended items with their scores\n",
    "# [{'movieId': 1, 'score': 0.5}, {'movieId': 2, 'score': 0.4}, {'movieId': 3, 'score': 0.3}]\n",
    "def make_content_based_recommendations(user, model) -> []:\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering - Neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should return a list of recommended items with their scores\n",
    "# [{'movieId': 1, 'score': 0.5}, {'movieId': 2, 'score': 0.4}, {'movieId': 3, 'score': 0.3}]\n",
    "def make_neighborhood_recommendations(user, model) -> []:\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering - Matrix Factorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should return a list of recommended items with their scores\n",
    "# [{'movieId': 1, 'score': 0.5}, {'movieId': 2, 'score': 0.4}, {'movieId': 3, 'score': 0.3}]\n",
    "def make_matrix_factorization_recommendations(user, model) -> []:\n",
    "    # add the user to the model\n",
    "    # get the recommendations for the user\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(user, content_model, collab_model1, collab_model2) -> []:\n",
    "    content_based_recommendations = make_content_based_recommendations(user, content_model)\n",
    "    neighborhood_recommendations = make_neighborhood_recommendations(user, collab_model1)\n",
    "    matrix_factorization_recommendations = make_matrix_factorization_recommendations(user, collab_model2)\n",
    "\n",
    "    # Combine the recommendations from the three models\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models to estimate a score for a recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user contains information about the user -> details tbd\n",
    "# probably a list of movies the user has rated and the ratings given\n",
    "\n",
    "# movie_list contains a list of movie ids\n",
    "def recommendations_from_list(user, movie_list, content_model, collab_model1, collab_model2):\n",
    "    scores = {}\n",
    "    for movie in movie_list:\n",
    "        try: \n",
    "            content_score = content_model.estimate(user, movie)\n",
    "        except: \n",
    "            content_score = 0\n",
    "        try:\n",
    "            collab_score1 = collab_model1.estimate(user, movie)\n",
    "        except: \n",
    "            collab_score1 = 0\n",
    "        try:\n",
    "            collab_score2 = collab_model2.estimate(user, movie)\n",
    "        except: \n",
    "            collab_score2 = 0\n",
    "\n",
    "        combined_score = (content_score + collab_score1 + collab_score2) / 3\n",
    "        scores[movie] = combined_score\n",
    "        # Create a combined score\n",
    "        combined_score = (content_score + collab_score1 + collab_score2) / 3\n",
    "        scores[movie] = combined_score\n",
    "    # Sort the scores with best recommendations first\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
